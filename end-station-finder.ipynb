{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End Station Finder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The challenge:\n",
    "My friend Lou has a project. They want to visit all the end stations on the Berlin S-Bahn and U-Bahn system. However, to count as a truly satisfying end station, the station must not only be at the end of its own line, but must also lack an interchange to another line. Lou wants only the end-iest of end stations.\n",
    "\n",
    "I want to find the end stations and present them to Lou in a format they can use for their project.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data:\n",
    "\n",
    "Berlin provides public transport data in the now-standard [GTFS](https://gtfs.org/) format, which stands for General (formerly Google) Transit Feed Specification. It is an [open standard](https://beyondtransparency.org/chapters/part-2/pioneering-open-data-standards-the-gtfs-story/) used by transit agencies to present their data in a uniform und useable way. It encompasses two types of data: realtime and static.\n",
    "- The realtime data is what drives (pun intended) our on-the-go transit apps and notifies us of delays and changes.\n",
    "- The static data (also known as *schedule data*) tells us and our apps about how all the points on the network are linked together. This is the GTFS data that will allow us identify the end stations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static GTFS data format\n",
    "GTFS is a data format in its own right, meant for \"[GTFS-consuming applications](https://www.transitwiki.org/TransitWiki/index.php/Category:GTFS-consuming_applications)\". However, conveniently, it is also easily readable in other software environements because it is merely a ZIP comprised of CSV (as TXT) files. The exact tables provided vary by agency. The following are the minimum required [table files for static GTFS](https://gtfs.org/schedule/reference/):\n",
    "-  `stops.txt` showing stops' name and location\n",
    "-  `stop_times.txt` showing every stop on every individual trip\n",
    "-  `trips.txt` showing trips' route, direction, destination* (headsign), and more\n",
    "-  `routes.txt` showing routes' transportation type, number, line colour, and more\n",
    "-  `agency.txt`\n",
    "\n",
    "My [GTFS download for Berlin](https://daten.berlin.de/datensaetze/vbb-fahrplandaten-gtfs) also contains these conditionally required and optional tables:\n",
    "-  `frequencies.txt` (note: empty), `calendar.txt` and `calendar_dates.txt` showing service frequency, day and date availability\n",
    "-  `pathways.txt` and `levels.txt` showing platform positions inside stations\n",
    "-  `shapes.txt` showing a trip's path, expressed as a sequence of co-ordinates \n",
    "-  `transfers.txt` showing the connections between stations and details about these transfers\n",
    "\n",
    "GTFS does not contain a simple list of `stops` on `route`. Instead, `stop_ID`s appear as sequences of `stop_times` in individual `trips`, which in turn belong to various `routes`. We need to join the IDs of stops, trips and routes to the (giant) `stop_times.txt`. \n",
    "\n",
    "I would therefore need to combine the following tables and fields to find the end stations if I were to make a database:\n",
    "\n",
    "`stop_times.txt` for the sequence of stops\n",
    "- \"trip_id\"         (primary key 1) (foreign key of trips.trip_id)\n",
    "- \"stop_sequence\"   (primary key 2)\n",
    "- \"arrival_time\"\n",
    "- \"departure_time\"\n",
    "- \"stop_id\" (foreign key of stops.stop_id)\n",
    "- \"pickup_type\"\n",
    "- \"drop_off_type\"\n",
    "\n",
    "`transfers.txt` for the transfers between different routes (or in our case, the lack thereof)\n",
    "\n",
    "- \"from_stop_id\"    (primary key 1) (foreign key of stops.stop_id)\n",
    "- \"to_stop_id\"      (primary key 2) (foreign key 2 of stops.stop_id)\n",
    "- \"from_route_id\"   (primary key 3) - might not be needed\n",
    "- \"to_route_id\"     (primary key 4) - might not be needed\n",
    "- \"transfer_type\"`  see [docs](https://gtfs.org/schedule/reference/#transferstxt)\n",
    "- \"min_transfer_time\" \n",
    "\n",
    "`trips.txt` for matching the trip IDs to routes\n",
    "- \"trip_id\"         (primary key)\n",
    "- \"route_id\"        (foreign key of routes.route_id)\n",
    "- \"trip_headsign\"   *\n",
    "- \"trip_short_name\" (can be route number but appears not to be used for train numbers here)\n",
    "- \"direction_id\"\n",
    "\n",
    "`stops.txt` for matching the stop IDs to stops\n",
    "- \"stop_id\"         (primary key)\n",
    "- \"stop_name\"       \n",
    "- \"stop_lon\"\n",
    "- \"stop_lat\"\n",
    "\n",
    "\\* the \"headsign\" destination is not to be confused with the end of a line; some trips do not travel the whole route. In our data this is a text field rather than an ID."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools for GTFS\n",
    "\n",
    "There are some [packages designed to handle GTFS data](https://gtfs.org/resources/gtfs) in Python, created by various developers. These are mainly used for geographical and frequency data. \n",
    "\n",
    "However, I found three that I may be able to use to find the end stations in **Python**, more easily than by manually creating a database:\n",
    "\n",
    "|    | Package | Data Structure | Potential Functions |\n",
    "| ---| ------- | -------------- | ------------------- |\n",
    "| 1.    | [`gtfs_kit`](https://mrcagney.github.io/gtfs_kit_docs/)   | Pandas  | `gtfs_kit.stops.get_stops` and `gtfs_kit.validators.check_transfers`|\n",
    "| 2. | [`transit_service_analyst`](https://github.com/psrc/transit_service_analyst/wiki/transit_service_analyst-documentation) | Pandas | `get_line_stops_gdf()` |\n",
    "| 3. | [`gtfs_utils`](https://open-bus-gtfs-utils.readthedocs.io/en/latest/route_stats.html)    | Pandas |  `route_stats` and `trip_stats`| \n",
    "\n",
    "I also noted down a few tools for **SQL**, in case the above don't work for me: \n",
    "1. [`node-gtfs`](https://github.com/BlinkTagInc/node-gtfs) SQLite\n",
    "2. [`gtfs-via-postgres`](https://github.com/public-transport/gtfs-via-postgres) PostgreSQL\n",
    "3. [`gtfs-schema`](https://github.com/tyleragreen/gtfs-schema) Schema builder\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `transit_service_analyst`\n",
    "### Step 1: Load packages, extensions and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import plotly.express as px\n",
    "import shapely.geometry\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to install and try the `transit_service_analyst` package first. It also requires `geopandas` and `pandera` to be installed. Now I can import the module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transit_service_analyst as tsa"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I used the command `tsa.load_gtfs(folder, service_date)` to try to load the data into a Pandas dataframe. The required arguments are:\n",
    "- the location of the unzipped GTFS folder\n",
    "- a sample date from the dataset in YYYYMMDD form. \n",
    "\n",
    "A date is needed because `tsa` constructs the routes based on the stops served in all scheduled trips on the sample day. A look in `calendar.txt` shows that my data runs from 20230615 to 20231209, so I picked today, 20230705, a Wednesday.\n",
    "\n",
    "**Note on date and route sampling issues:**\n",
    "Choosing only a single date to build the routes is a potential issue because there is the possibility that it might not sample the whole length of routes. For example, in the case of construction work that prevents trains reaching their normal end station, the scheduled trip data might show an point earlier than the end of the physical route. This can be partially controlled for using information already contained in Berlin's GTFS data. Indeed, today, like most or maybe all days, is marked as an exception in `calendar_dates.txt`. Later I will check the affected `service_id`s to find out if they are relevant. However, this would not account for any shortened routes that span enough time to be counted as the normal schedule rather than the exception, and I will attempt to overcome this later, probably by sampling various dates from the set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transit_analyst = tsa.load_gtfs('Berlin GTFS data', '20230705')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i got an error: `Cannot set a DataFrame with multiple columns to the single column start_time_secs` [and other _time_secs columns]\n",
    "#attempting to debug and reload. \n",
    "#Autoreload lets me use the changes instantly without a kernel reload.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Failures due to data compatibility issues\n",
    "\n",
    "#### Empty csv\n",
    "Unfortunately it seems like this package requires a populated `frequencies.txt` to gather the `trip_id`s. This csv is however empty in Berlin's static data. The code in `gtfs_service.py` was set up to deal with the file existing or not existing, but not being empty:\n",
    "\n",
    "```python\n",
    "if os.path.exists(os.path.join(self.gtfs_dir, \"frequencies.txt\")):\n",
    "            self.trips, self.stop_times = self.frequencies_to_trips()\n",
    "```\n",
    "Solved for now by renaming the file, but it would be good to suggest a change on Github later.\n",
    "\n",
    "#### Inconsistent `route_type`s\n",
    "Berlin's data uses [Google Transit extended route types](https://developers.google.com/transit/gtfs/reference/extended-route-types) rather than the classic [GTFS route types](https://gtfs.org/schedule/reference/#routestxt) so Pandera's schema validation fails:\n",
    "\n",
    "```console\n",
    "    Error Counts\n",
    "    ------------\n",
    "    - SchemaErrorReason.SCHEMA_COMPONENT_CHECK: 1\n",
    "    \n",
    "    Schema Error Summary\n",
    "    --------------------\n",
    "                                                                                        failure_cases  n_failure_cases\n",
    "    schema_context column     check                                                                                   \n",
    "    Column         route_type isin([0, 1, 2, 3, 4, 5, 6, 7, 11, 12])  [700, 100, 900, 1000, 109, 400]                6\n",
    "```\n",
    "\n",
    "Can I easily update the schema?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/transit_service_analyst/gtfs_service.py:95: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  trips_df = pd.read_csv(os.path.join(self.gtfs_dir, \"trips.txt\"))\n",
      "/home/sian/anaconda3/envs/stats_env/lib/python3.9/site-packages/transit_service_analyst/gtfs_service.py:116: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  stop_times_df = pd.read_csv(os.path.join(self.gtfs_dir, \"stop_times.txt\"))\n"
     ]
    },
    {
     "ename": "SchemaErrors",
     "evalue": "Schema Routes: A total of 1 schema errors were found.\n\nError Counts\n------------\n- SchemaErrorReason.SCHEMA_COMPONENT_CHECK: 1\n\nSchema Error Summary\n--------------------\n                                                                                    failure_cases  n_failure_cases\nschema_context column     check                                                                                   \nColumn         route_type isin([0, 1, 2, 3, 4, 5, 6, 7, 11, 12])  [700, 100, 900, 1000, 109, 400]                6\n\nUsage Tip\n---------\n\nDirectly inspect all errors by catching the exception:\n\n```\ntry:\n    schema.validate(dataframe, lazy=True)\nexcept SchemaErrors as err:\n    err.failure_cases  # dataframe of schema errors\n    err.data  # invalid dataframe\n```\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSchemaErrors\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m transit_analyst \u001b[39m=\u001b[39m tsa\u001b[39m.\u001b[39;49mload_gtfs(\u001b[39m'\u001b[39;49m\u001b[39mBerlin GTFS data\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m20230705\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/stats_env/lib/python3.9/site-packages/transit_service_analyst/load_gtfs.py:5\u001b[0m, in \u001b[0;36mload_gtfs\u001b[0;34m(gtfs_dir, service_date)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_gtfs\u001b[39m(gtfs_dir, service_date):\n\u001b[0;32m----> 5\u001b[0m     gtfs_service \u001b[39m=\u001b[39m Service_Utils(gtfs_dir, service_date)\n\u001b[1;32m      6\u001b[0m     \u001b[39mreturn\u001b[39;00m gtfs_service\n",
      "File \u001b[0;32m~/anaconda3/envs/stats_env/lib/python3.9/site-packages/transit_service_analyst/gtfs_service.py:38\u001b[0m, in \u001b[0;36mService_Utils.__init__\u001b[0;34m(self, gtfs_dir, service_date)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrips, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_times \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfrequencies_to_trips()\n\u001b[1;32m     37\u001b[0m \u001b[39m# self._df_all_stops_by_trips = self.__get_trips_stop_times()\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroutes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_routes()\n\u001b[1;32m     39\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_times[\u001b[39m\"\u001b[39m\u001b[39mstop_id\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39munique()\n\u001b[1;32m     40\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstops \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_stops()\n",
      "File \u001b[0;32m~/anaconda3/envs/stats_env/lib/python3.9/site-packages/transit_service_analyst/gtfs_service.py:107\u001b[0m, in \u001b[0;36mService_Utils.__get_routes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[39mGets records in routes.txt for the trips that represent the\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[39mservice_date passed into the constructor. Returns a DataFrame.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    106\u001b[0m routes_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgtfs_dir, \u001b[39m\"\u001b[39m\u001b[39mroutes.txt\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m--> 107\u001b[0m routes_df \u001b[39m=\u001b[39m GTFS_Schema\u001b[39m.\u001b[39;49mRoutes\u001b[39m.\u001b[39;49mvalidate(routes_df, lazy\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    108\u001b[0m routes_df \u001b[39m=\u001b[39m routes_df[routes_df[\u001b[39m\"\u001b[39m\u001b[39mroute_id\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39misin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrips[\u001b[39m\"\u001b[39m\u001b[39mroute_id\u001b[39m\u001b[39m\"\u001b[39m])]\n\u001b[1;32m    109\u001b[0m \u001b[39mreturn\u001b[39;00m routes_df\n",
      "File \u001b[0;32m~/anaconda3/envs/stats_env/lib/python3.9/site-packages/pandera/api/pandas/model.py:304\u001b[0m, in \u001b[0;36mDataFrameModel.validate\u001b[0;34m(cls, check_obj, head, tail, sample, random_state, lazy, inplace)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    290\u001b[0m \u001b[39m@docstring_substitution\u001b[39m(validate_doc\u001b[39m=\u001b[39mDataFrameSchema\u001b[39m.\u001b[39mvalidate\u001b[39m.\u001b[39m\u001b[39m__doc__\u001b[39m)\n\u001b[1;32m    291\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalidate\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    299\u001b[0m     inplace: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    300\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrameBase[TDataFrameModel]:\n\u001b[1;32m    301\u001b[0m     \u001b[39m\"\"\"%(validate_doc)s\"\"\"\u001b[39;00m\n\u001b[1;32m    302\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(\n\u001b[1;32m    303\u001b[0m         DataFrameBase[TDataFrameModel],\n\u001b[0;32m--> 304\u001b[0m         \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mto_schema()\u001b[39m.\u001b[39;49mvalidate(\n\u001b[1;32m    305\u001b[0m             check_obj, head, tail, sample, random_state, lazy, inplace\n\u001b[1;32m    306\u001b[0m         ),\n\u001b[1;32m    307\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/stats_env/lib/python3.9/site-packages/pandera/api/pandas/container.py:341\u001b[0m, in \u001b[0;36mDataFrameSchema.validate\u001b[0;34m(self, check_obj, head, tail, sample, random_state, lazy, inplace)\u001b[0m\n\u001b[1;32m    329\u001b[0m     check_obj \u001b[39m=\u001b[39m check_obj\u001b[39m.\u001b[39mmap_partitions(  \u001b[39m# type: ignore [operator]\u001b[39;00m\n\u001b[1;32m    330\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate,\n\u001b[1;32m    331\u001b[0m         head\u001b[39m=\u001b[39mhead,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    337\u001b[0m         meta\u001b[39m=\u001b[39mcheck_obj,\n\u001b[1;32m    338\u001b[0m     )\n\u001b[1;32m    339\u001b[0m     \u001b[39mreturn\u001b[39;00m check_obj\u001b[39m.\u001b[39mpandera\u001b[39m.\u001b[39madd_schema(\u001b[39mself\u001b[39m)\n\u001b[0;32m--> 341\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate(\n\u001b[1;32m    342\u001b[0m     check_obj\u001b[39m=\u001b[39;49mcheck_obj,\n\u001b[1;32m    343\u001b[0m     head\u001b[39m=\u001b[39;49mhead,\n\u001b[1;32m    344\u001b[0m     tail\u001b[39m=\u001b[39;49mtail,\n\u001b[1;32m    345\u001b[0m     sample\u001b[39m=\u001b[39;49msample,\n\u001b[1;32m    346\u001b[0m     random_state\u001b[39m=\u001b[39;49mrandom_state,\n\u001b[1;32m    347\u001b[0m     lazy\u001b[39m=\u001b[39;49mlazy,\n\u001b[1;32m    348\u001b[0m     inplace\u001b[39m=\u001b[39;49minplace,\n\u001b[1;32m    349\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/stats_env/lib/python3.9/site-packages/pandera/api/pandas/container.py:371\u001b[0m, in \u001b[0;36mDataFrameSchema._validate\u001b[0;34m(self, check_obj, head, tail, sample, random_state, lazy, inplace)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_inferred:\n\u001b[1;32m    363\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    364\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThis \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m is an inferred schema that hasn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt been \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    365\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmodified. It\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms recommended that you refine the schema \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[39mUserWarning\u001b[39;00m,\n\u001b[1;32m    369\u001b[0m     )\n\u001b[0;32m--> 371\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_backend(check_obj)\u001b[39m.\u001b[39;49mvalidate(\n\u001b[1;32m    372\u001b[0m     check_obj,\n\u001b[1;32m    373\u001b[0m     schema\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    374\u001b[0m     head\u001b[39m=\u001b[39;49mhead,\n\u001b[1;32m    375\u001b[0m     tail\u001b[39m=\u001b[39;49mtail,\n\u001b[1;32m    376\u001b[0m     sample\u001b[39m=\u001b[39;49msample,\n\u001b[1;32m    377\u001b[0m     random_state\u001b[39m=\u001b[39;49mrandom_state,\n\u001b[1;32m    378\u001b[0m     lazy\u001b[39m=\u001b[39;49mlazy,\n\u001b[1;32m    379\u001b[0m     inplace\u001b[39m=\u001b[39;49minplace,\n\u001b[1;32m    380\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/stats_env/lib/python3.9/site-packages/pandera/backends/pandas/container.py:126\u001b[0m, in \u001b[0;36mDataFrameSchemaBackend.validate\u001b[0;34m(self, check_obj, schema, head, tail, sample, random_state, lazy, inplace)\u001b[0m\n\u001b[1;32m    119\u001b[0m         error_handler\u001b[39m.\u001b[39mcollect_error(\n\u001b[1;32m    120\u001b[0m             result\u001b[39m.\u001b[39mreason_code,\n\u001b[1;32m    121\u001b[0m             error,\n\u001b[1;32m    122\u001b[0m             original_exc\u001b[39m=\u001b[39mresult\u001b[39m.\u001b[39moriginal_exc,\n\u001b[1;32m    123\u001b[0m         )\n\u001b[1;32m    125\u001b[0m \u001b[39mif\u001b[39;00m error_handler\u001b[39m.\u001b[39mcollected_errors:\n\u001b[0;32m--> 126\u001b[0m     \u001b[39mraise\u001b[39;00m SchemaErrors(\n\u001b[1;32m    127\u001b[0m         schema\u001b[39m=\u001b[39mschema,\n\u001b[1;32m    128\u001b[0m         schema_errors\u001b[39m=\u001b[39merror_handler\u001b[39m.\u001b[39mcollected_errors,\n\u001b[1;32m    129\u001b[0m         data\u001b[39m=\u001b[39mcheck_obj,\n\u001b[1;32m    130\u001b[0m     )\n\u001b[1;32m    132\u001b[0m \u001b[39mreturn\u001b[39;00m check_obj\n",
      "\u001b[0;31mSchemaErrors\u001b[0m: Schema Routes: A total of 1 schema errors were found.\n\nError Counts\n------------\n- SchemaErrorReason.SCHEMA_COMPONENT_CHECK: 1\n\nSchema Error Summary\n--------------------\n                                                                                    failure_cases  n_failure_cases\nschema_context column     check                                                                                   \nColumn         route_type isin([0, 1, 2, 3, 4, 5, 6, 7, 11, 12])  [700, 100, 900, 1000, 109, 400]                6\n\nUsage Tip\n---------\n\nDirectly inspect all errors by catching the exception:\n\n```\ntry:\n    schema.validate(dataframe, lazy=True)\nexcept SchemaErrors as err:\n    err.failure_cases  # dataframe of schema errors\n    err.data  # invalid dataframe\n```\n"
     ]
    }
   ],
   "source": [
    "transit_analyst = tsa.load_gtfs('Berlin GTFS data', '20230705')\n",
    "#routes_df failed Pandera schema validation and yielded another error so I added lazy=True to all dfs' validation in the source "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stats_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
